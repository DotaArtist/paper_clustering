{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5425696e4485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# {paper_id:{id:'',tittle:'',abstract:'<长文本>',keywords:[],authors:[{name:'',org:''},venue:'',year:'']}}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# authors: 中文名LI Heng\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_pub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_pub_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# {authors:[id,...]} hengli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/sna_data/sna_valid_pub.json'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/sna_data/sna_valid_pub.json'",
     "output_type": "error"
    }
   ],
   "source": [
    "# {paper_id:{id:'',tittle:'',abstract:'<长文本>',keywords:[],authors:[{name:'',org:''},venue:'',year:'']}} \n",
    "# authors: 中文名LI Heng\n",
    "test_pub = json.load(open(config.test_pub_path, mode='r', encoding='utf-8'))\n",
    "\n",
    "# {authors:[id,...]} hengli\n",
    "test_author = json.load(open(config.test_author_path, mode='r', encoding='utf-8'))\n",
    "\n",
    "train_pub = json.load(open(config.train_pub_path, mode='r', encoding='utf-8'))\n",
    "train_author = json.load(open(config.train_author_path, mode='r', encoding='utf-8'))\n",
    "\n",
    "# {authors:[[id,...],[],[]]}\n",
    "test_cluster = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "org_name, authors_name = [], []\n",
    "\n",
    "for i in test_pub:\n",
    "    for j in test_pub[i]:\n",
    "        for k in test_pub[i]['authors']:\n",
    "            if \"org\" in k.keys():\n",
    "                if k['org'] != \"\":\n",
    "                    org_name.append(k['org'])\n",
    "                else:\n",
    "                    org_name.append('NO_ORG')\n",
    "                authors_name.append(k['name'])\n",
    "            else:\n",
    "                org_name.append('NO_ORG')\n",
    "                authors_name.append(k['name'])\n",
    "\n",
    "org_authors_statics = pd.DataFrame({'org':org_name, 'authors':authors_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from bert_pre_train import BertPreTrain\n",
    "pre_train_model = BertPreTrain(mode='pre_train', language='en', padding=False,\n",
    "                                            embedding_dim=config.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 统计每个聚类里文章的数量\n",
    "statistic_paper_num = []\n",
    "for _name in train_author.keys():\n",
    "    for _cluster_id in  train_author[_name]:\n",
    "        statistic_paper_num.append(len(train_author[_name][_cluster_id]))\n",
    "_ = pd.Series(statistic_paper_num)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "print(_.value_counts())\n",
    "print(len(_.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "keyword_dict = pickle.load(open('../my_data/keyword_feature_map_dict.pkl', mode='rb'))\n",
    "# with open('../my_data/keywod_embedding_可视化.txt', mode='w', encoding='utf-8') as fk:\n",
    "#     for _keyword in keyword_dict.keys():\n",
    "#         a = pre_train_model.get_output([_keyword], _show_tokens=False).astype('str').tolist()[0][0]\n",
    "#         fk.writelines('{}\\n'.format('\\t'.join(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "keyword_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "org_authors_statics[org_authors_statics['authors'] == 'Hao Zhang']['org'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# origin_data: 文章属性信息; content_data：文章内容信息\n",
    "with open(config.origin_data, mode='w', encoding='utf-8') as f1:\n",
    "    with open(config.content_data, mode='w', encoding='utf-8') as f2:\n",
    "        for paper_id in train_pub:\n",
    "            # paper_id, name_org, tittle, content, keywords, venue, year\n",
    "            name_org = []\n",
    "            for k in train_pub[paper_id]['authors']:\n",
    "                if \"org\" in k.keys():\n",
    "                    if k['org'] != \"\":\n",
    "                        name_org.append(k['name'] + '@' + k['org'])\n",
    "                    else:\n",
    "                        name_org.append(k['name'] + '@' + 'NO_ORG')\n",
    "                else:\n",
    "                    name_org.append(k['name'] + '@' + 'NO_ORG')\n",
    "            name_org = '|'.join(name_org)\n",
    "            if isinstance(name_org, type(None)):\n",
    "                name_org = ''\n",
    "\n",
    "            tittle = train_pub[paper_id]['title']\n",
    "            if isinstance(tittle, type(None)):\n",
    "                tittle = ''\n",
    "\n",
    "            if 'abstract' in train_pub[paper_id].keys():\n",
    "                content = train_pub[paper_id]['abstract']\n",
    "            else:\n",
    "                content = ''\n",
    "            if isinstance(content, type(None)):\n",
    "                content = ''\n",
    "\n",
    "            if 'keywords' in train_pub[paper_id].keys():\n",
    "                keywords = train_pub[paper_id]['keywords']\n",
    "                keywords = '|'.join(keywords)\n",
    "            else:\n",
    "                keywords = ''\n",
    "            if isinstance(keywords, type(None)):\n",
    "                keywords = ''\n",
    "\n",
    "            venue = train_pub[paper_id]['venue']\n",
    "\n",
    "            if 'year' in train_pub[paper_id].keys():\n",
    "                year = train_pub[paper_id]['year']\n",
    "            else:\n",
    "                year = ''\n",
    "            f1.writelines('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(paper_id,\n",
    "                                                                name_org.replace('\\r', '').replace('\\n','').strip(),\n",
    "                                                                tittle.replace('\\r', '').replace('\\n','').strip(),\n",
    "                                                                content.replace('\\r', '').replace('\\n', '').strip(),\n",
    "                                                                keywords.replace('\\r', '').replace('\\n', '').strip(), venue, year))\n",
    "            f2.writelines('{}\\n'.format(content.replace('\\r', '').replace('\\n', '').strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# origin_test_data: 文章属性信息; content_test_data：文章内容信息\n",
    "with open(config.test_origin_data, mode='w', encoding='utf-8') as f1:\n",
    "    with open('../my_data/content_test.txt', mode='w', encoding='utf-8') as f2:\n",
    "        for paper_id in test_pub:\n",
    "            # paper_id, name_org, tittle, content, keywords, venue, year\n",
    "            name_org = []\n",
    "            for k in test_pub[paper_id]['authors']:\n",
    "                if \"org\" in k.keys():\n",
    "                    if k['org'] != \"\":\n",
    "                        name_org.append(k['name'] + '@' + k['org'])\n",
    "                    else:\n",
    "                        name_org.append(k['name'] + '@' + 'NO_ORG')\n",
    "                else:\n",
    "                    name_org.append(k['name'] + '@' + 'NO_ORG')\n",
    "            name_org = '|'.join(name_org)\n",
    "            if isinstance(name_org, type(None)):\n",
    "                name_org = ''\n",
    "\n",
    "            tittle = test_pub[paper_id]['title']\n",
    "            if isinstance(tittle, type(None)):\n",
    "                tittle = ''\n",
    "\n",
    "            if 'abstract' in test_pub[paper_id].keys():\n",
    "                content = test_pub[paper_id]['abstract']\n",
    "            else:\n",
    "                content = ''\n",
    "            if isinstance(content, type(None)):\n",
    "                content = ''\n",
    "\n",
    "            if 'keywords' in test_pub[paper_id].keys():\n",
    "                keywords = test_pub[paper_id]['keywords']\n",
    "                keywords = '|'.join(keywords)\n",
    "            else:\n",
    "                keywords = ''\n",
    "            if isinstance(keywords, type(None)):\n",
    "                keywords = ''\n",
    "\n",
    "            venue = test_pub[paper_id]['venue']\n",
    "\n",
    "            if 'year' in test_pub[paper_id].keys():\n",
    "                year = test_pub[paper_id]['year']\n",
    "            else:\n",
    "                year = ''\n",
    "            f1.writelines('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(paper_id,\n",
    "                                                                name_org.replace('\\r', '').replace('\\n','').strip(),\n",
    "                                                                tittle.replace('\\r', '').replace('\\n','').strip(),\n",
    "                                                                content.replace('\\r', '').replace('\\n', '').strip(),\n",
    "                                                                keywords.replace('\\r', '').replace('\\n', '').strip(), venue, year))\n",
    "            f2.writelines('{}\\n'.format(content.replace('\\r', '').replace('\\n', '').strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(train_pub['LAKfuSaP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_author[list(train_author.keys())[0]]\n",
    "\n",
    "def merge_paper_list(_dict_a):\n",
    "    # 合并训练集中，同一作者下的文章，返回文章列表\n",
    "    out_list = []\n",
    "    for i in _dict_a:\n",
    "        out_list.extend(_dict_a[i])\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a = pickle.load(open('../my_data/author_feature_map.pkl', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# 某作者下的文章\n",
    "out_list = merge_paper_list(train_author[list(train_author.keys())[0]])\n",
    "out_list_feature = dict()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sim_matrix = np.zeros((len(out_list),len(out_list)))\n",
    "\n",
    "with open('../my_data/author_feature_simple.txt', mode='r', encoding='utf-8') as fa:\n",
    "    for line in fa.readlines():\n",
    "        paper_id, feature = line.strip().split('\\t')\n",
    "        feature_set = set(feature.split('|'))\n",
    "        \n",
    "        if paper_id in out_list:\n",
    "            out_list_feature[paper_id] = feature_set\n",
    "    \n",
    "    for id_a, paper_id_a in enumerate(out_list):\n",
    "        for id_b, paper_id_b in enumerate(out_list):\n",
    "            sim_matrix[id_a, id_b] = len(out_list_feature[paper_id_a] & out_list_feature[paper_id_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "out_list = merge_paper_list(train_author[list(train_author.keys())[0]])\n",
    "print(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for idx, i in enumerate(list(sim_matrix[out_list.index('FTweBQNS')])):\n",
    "    if i >= 2:\n",
    "        print(out_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def read_paper_embedding():\n",
    "    with open('train_paper_embedding.txt', mode='r', encoding='utf-8') as f1:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "[len(train_author[list(train_author.keys())[i]]) for i in range(len(list(train_author.keys())))].index(588)\n",
    "train_author[list(train_author.keys())[203]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_pub['wlMo36J1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_pub['dfcJQvnq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# tf-idf embediing\n",
    "import re\n",
    "import dill as pickle\n",
    "tfidf_model = pickle.load(open('../my_data/tf_idf_model.pkl', mode=\"rb\"))\n",
    "\n",
    "with open(config.origin_data, mode='r', encoding='utf-8') as fo:\n",
    "    paper_id_list = []\n",
    "    paper_content_list = []\n",
    "    for (line_index, line) in enumerate(fo):\n",
    "        line = line.strip('\\n').split('\\t')\n",
    "        paper_id, tittle, content, keywords = line[0], line[2], line[3], line[4]\n",
    "        paper_id_list.append(paper_id)\n",
    "        paper_content_list.append(' '.join([tittle, content, keywords]))\n",
    "\n",
    "x_train = tfidf_model.transform(paper_content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "'microstructure' in tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(list(keyword_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_author['']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}